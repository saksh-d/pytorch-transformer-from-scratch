{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac2d36a",
   "metadata": {},
   "source": [
    "# Transformer using PyTorch\n",
    "\n",
    "Replicating the paper \"Attention Is All You Need\" from scratch using PyTorch.\n",
    "\n",
    "Available here: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdc492",
   "metadata": {},
   "source": [
    "## Building the Encoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54f417",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "In NLP, words/tokens are represented as integers (token IDs), but since neural networks cannot work directly with numbers, these are first converted into continuous dense vectors via embeddings.\n",
    "\n",
    "The embedding layer:\n",
    "- Maps each token ID to a learnable vector of dimension `d_model`\n",
    "- Allows the model to learn semantic meaning - similar words/tokens get similar embeddings during training\n",
    "\n",
    "Refer to section 3.4 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #maps token IDs to vectors\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of token indices, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Embedded tensor, scaled, shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)  #Refer section 3.4 of the paper (This is primarily done to balance the scale of the embeddings and positional encodings, and helps stabilize training early on.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238030c",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Transformers process tokens in parallel, with no built-in sense of order. But language is sequential:\n",
    "\n",
    "\"The cat sat on the mat\" =/= \"The mat cat on the sat\"\n",
    "\n",
    "Positional information is injected into the input to create awareness of the token position, using Positional Encoding. They have the same dimension `d_model` as the input embeddings.\n",
    "\n",
    "Refer to section 3.5 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3212ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Create matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #Create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) #tensor contains positions 0,1,...,seq_len - 1\n",
    "        #Using section 3.5 to write out position encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #sine to even positions\n",
    "        pe[:, 1::2] = torch.sin(position * div_term) #cosine to odd positions\n",
    "\n",
    "        #Add batch dimension to make the shape (1, seq_len, d_model), so it can be added to x during forward pass\n",
    "        #with shape (batch_size, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe) #tensor gets saved with the model and moves with it to GPU/CPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape(1), :]).requires_grad_(False) #Add positional encodings to embeddings, requires_grad_() False means that this won't participate in backpropagation\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b617ed",
   "metadata": {},
   "source": [
    "### Add + Norm Layer\n",
    "\n",
    "- Layer Norm normalizes each sampleâ€™s hidden vector across dimensions, improving training dynamics and convergence.\n",
    "- LayerNorm is preferred over BatchNorm because:\n",
    "    1. It doesn't depend on batch statistics.\n",
    "    2. Works well for variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12840f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #added (shift)\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) #multiplied (scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias #From section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d4584",
   "metadata": {},
   "source": [
    "### Feed Forward layer\n",
    "\n",
    "Discussed in section 3.3 of the paper, and equation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ac9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) #W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) #W2 and B2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x)))) #(batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023eb53",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Discussed in section 3.2.2 of the paper, this is the core innovation of the transformer model.\n",
    "It is multiple scaled dot-product attention heads run in parallel, followed by a linear projection.\n",
    "\n",
    "Refer to equation 1 for calculating attention scores, and then two equations in section 3.2.2 for multihead attention scores\n",
    "\n",
    "Steps involved:\n",
    "1. Project input `(Q, K, V)` into h subspaces using `W_q`, `W_k`, `W_v`.\n",
    "2. Compute scaled dot-product attention for each head.\n",
    "3. Concatenate all head outputs.\n",
    "4. Pass through a final `W_o` projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout): #h is the number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model #we have to ensure that d_model is divisible by h, d_model/h = d_k\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model) #W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) #W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) #W_v\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) #W_o\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1] #last dimension of Q,K,V\n",
    "\n",
    "        # Calculate attention using Equation 1\n",
    "        # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) #matrix multiplication of Q with K.transpose\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) #values we don't want in the attention matrix will be replaced by a very low value\n",
    "        \n",
    "        # apply softmax\n",
    "        attention_scores = attention_scores.softmax(dim = -1) #(batch, h, seq_len, seq_len)\n",
    "\n",
    "        # apply dropout\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)       # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    " \n",
    "        # Reshaping for MultiHead Attention from [batch, seq_len, d_model] to [batch, seq_len, h, d_k]\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) #revert the transpose operation (batch, h, seq_len, d_k).transpose -> (batch, seq_len, d_model)\n",
    "        \n",
    "        #(batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fad4",
   "metadata": {},
   "source": [
    "### Create skip connection (residual connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdb0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
