{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac2d36a",
   "metadata": {},
   "source": [
    "# Transformer from Scratch - English to Norwegian Translation\n",
    "\n",
    "This notebook implements a full Transformer architecture from the ground up in PyTorch, inspired by the seminal paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762). The model is trained on the `en-no` subset of the [Helsinki-NLP / opus_books](https://huggingface.co/datasets/Helsinki-NLP/opus_books) dataset to perform English → Norwegian translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchmetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3cd35",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834aa540",
   "metadata": {},
   "outputs": [],
   "source": [
    "## config.py\n",
    "\n",
    "def get_config():\n",
    "    return{\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 0.0001,\n",
    "        \"seq_len\": 150,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",  #set source language here\n",
    "        \"lang_tgt\": \"no\",  #set target language here\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"transformer1_\",\n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/transformer_model\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "\n",
    "    return str(Path('.') / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdc492",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54f417",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "In NLP, words/tokens are represented as integers (token IDs), but since neural networks cannot work directly with numbers, these are first converted into continuous dense vectors via embeddings.\n",
    "\n",
    "The embedding layer:\n",
    "- Maps each token ID to a learnable vector of dimension `d_model`\n",
    "- Allows the model to learn semantic meaning - similar words/tokens get similar embeddings during training\n",
    "\n",
    "Refer to section 3.4 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c8d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #maps token IDs to vectors\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of token indices, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Embedded tensor, scaled, shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)  #Refer section 3.4 of the paper (This is primarily done to balance the scale of the embeddings and positional encodings, and helps stabilize training early on.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238030c",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Transformers process tokens in parallel, with no built-in sense of order. But language is sequential:\n",
    "\n",
    "\"The cat sat on the mat\" =/= \"The mat cat on the sat\"\n",
    "\n",
    "Positional information is injected into the input to create awareness of the token position, using Positional Encoding. They have the same dimension `d_model` as the input embeddings.\n",
    "\n",
    "Refer to section 3.5 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3212ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Create matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #Create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) #tensor contains positions 0,1,...,seq_len - 1\n",
    "        #Using section 3.5 to write out position encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #sine to even positions\n",
    "        pe[:, 1::2] = torch.sin(position * div_term) #cosine to odd positions\n",
    "\n",
    "        #Add batch dimension to make the shape (1, seq_len, d_model), so it can be added to x during forward pass\n",
    "        #with shape (batch_size, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe) #tensor gets saved with the model and moves with it to GPU/CPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) #Add positional encodings to embeddings, requires_grad_() False means that this won't participate in backpropagation\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b617ed",
   "metadata": {},
   "source": [
    "### Add + Norm Layer\n",
    "\n",
    "- Layer Norm normalizes each sample’s hidden vector across dimensions, improving training dynamics and convergence.\n",
    "- LayerNorm is preferred over BatchNorm because:\n",
    "    1. It doesn't depend on batch statistics.\n",
    "    2. Works well for variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12840f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #added (shift)\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) #multiplied (scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias #From section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d4584",
   "metadata": {},
   "source": [
    "### Feed Forward layer\n",
    "\n",
    "Discussed in section 3.3 of the paper, and equation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20ac9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) #W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) #W2 and B2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x)))) #(batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023eb53",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Discussed in section 3.2.2 of the paper, this is the core innovation of the transformer model.\n",
    "It is multiple scaled dot-product attention heads run in parallel, followed by a linear projection.\n",
    "\n",
    "Refer to equation 1 for calculating attention scores, and then two equations in section 3.2.2 for multihead attention scores\n",
    "\n",
    "Steps involved:\n",
    "1. Project input `(Q, K, V)` into h subspaces using `W_q`, `W_k`, `W_v`.\n",
    "2. Compute scaled dot-product attention for each head.\n",
    "3. Concatenate all head outputs.\n",
    "4. Pass through a final `W_o` projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c6cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout): #h is the number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model #we have to ensure that d_model is divisible by h, d_model/h = d_k\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model) #W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) #W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) #W_v\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) #W_o\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1] #last dimension of Q,K,V\n",
    "\n",
    "        # Calculate attention using Equation 1\n",
    "        # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) #matrix multiplication of Q with K.transpose\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) #values we don't want in the attention matrix will be replaced by a very low value\n",
    "        \n",
    "        # apply softmax\n",
    "        attention_scores = attention_scores.softmax(dim = -1) #(batch, h, seq_len, seq_len)\n",
    "\n",
    "        # apply dropout\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)       # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    " \n",
    "        # Reshaping for MultiHead Attention from [batch, seq_len, d_model] to [batch, seq_len, h, d_k]\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) #revert the transpose operation (batch, h, seq_len, d_k).transpose -> (batch, seq_len, d_model)\n",
    "        \n",
    "        #(batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fad4",
   "metadata": {},
   "source": [
    "### Create skip connection (residual connection)\n",
    "\n",
    "Addition is element-wise, and the result layer is normalized\n",
    "\n",
    "`Output = LayerNorm(x + Sublayer(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffdb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    #Applying Post-Norm here\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e6c1d",
   "metadata": {},
   "source": [
    "## Creating Encoder block\n",
    "Creating the Encoder block, and stacking N of them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_block: MultiHeadAttention, \n",
    "                 feed_forward_block: FeedForward,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([SkipConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask): #mask to hide padding words\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e7090",
   "metadata": {},
   "source": [
    "### Creating the Encoder object (N encoder blocks stacked)\n",
    "\n",
    "Each block:\n",
    "- Applies multi-head attention to every position with all other positions (self-attention)\n",
    "- Applies a feedforward network\n",
    "- Each sub-layer is wrapped with residual + layer norm\n",
    "\n",
    "After all blocks, it applies a final layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef379cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:  #Loop over each EncoderBlock in self.layers\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747610e",
   "metadata": {},
   "source": [
    "## Creating Decoder Block\n",
    "\n",
    "Uses the same classes as the Encoder block for the most part.\n",
    "The key difference comes in with the Masked Multi-Head Attention (uses self-attention), and an additional, cross-attention Multi-Head Attention block.\n",
    "\n",
    "Refer to section 3.1 in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c494dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_block: MultiHeadAttention,\n",
    "                 cross_attention_block: MultiHeadAttention,\n",
    "                 feed_forward_block: FeedForward,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        self.residual_connections = nn.ModuleList([SkipConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): #src_mask: encoder, tgt_mask: decoder\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask)) #self-attention block of decoder\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)) #cross-attention takes in key from the decoder, and the query and value from the encoder, with the encoder mask\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc81f44",
   "metadata": {},
   "source": [
    "### Creating Decoder object (N decoder blocks stacked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50318473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76031cb8",
   "metadata": {},
   "source": [
    "## Final Linear Layer\n",
    "Projects the embeddings into a position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2270133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #(batch, seq_len, d_model) -> (batch, seq_len, vocab_size)\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe16925",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "We now have all the components we need to put together and create the Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e693bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embed: InputEmbeddings,\n",
    "                 tgt_embed: InputEmbeddings,\n",
    "                 src_pos: PositionalEncoding,\n",
    "                 tgt_pos: PositionalEncoding,\n",
    "                 projection_layer: ProjectionLayer\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    #define 3 methods: one to encode, one to decode, one to project\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)   #apply embedding\n",
    "        src = self.src_pos(src)     #apply positional encoding\n",
    "        return self.encoder(src, src_mask)  #apply encoder block\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)   #apply embedding\n",
    "        tgt = self.tgt_pos(tgt)     #apply positional encoding\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c456771",
   "metadata": {},
   "source": [
    "### Function to build transformer\n",
    "\n",
    "Given all hyperparameters, this function builds the transformer and initializes the parameters with some initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a86e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int,\n",
    "                      tgt_vocab_size: int,\n",
    "                      src_seq_len: int,\n",
    "                      tgt_seq_len: int,\n",
    "                      d_model: int = 512,   #as mentioned in the paper\n",
    "                      N: int = 6,           #number of encoder/decoder blocks\n",
    "                      h: int = 8,           #number of heads\n",
    "                      dropout: float = 0.1,\n",
    "                      d_ff: int = 2048\n",
    ") -> Transformer:\n",
    "    \n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    #create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    #create decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, \n",
    "                                     decoder_cross_attention_block,\n",
    "                                     feed_forward_block,\n",
    "                                     dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    #Create the encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #Create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    #Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    #Initialize parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f382d",
   "metadata": {},
   "source": [
    "# Creating English - Norwegian translation Transformer\n",
    "\n",
    "1. Using opus_books dataset from HuggingFace (Available here: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-es?views%5B%5D=en_es)\n",
    "2. Use word-level tokenizer from HuggingFace - Splits sentence into tokens (Build vocabulary) (Documentation here: https://github.com/huggingface/tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc443f",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096185d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, lang):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def build_tokenizer(config, dataset, lang):\n",
    "    #config['tokenizer_file'] = '../tokenizers/tokenizer_{0}.json'\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) #tokenizes unknown word\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2) #unknown word, padding, start of sentenece, end of sentence | min_frequency: for the word to appear in the vocabulary it needs to appear at least twice\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a24c9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Custom Dataset Class\n",
    "\n",
    "Use a custom Dataset class, following PyTorch documentation: https://docs.pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec9919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        #Save tokens to create tensors for the model: SOS, EOS and Padding\n",
    "        #Use special method of tokenizer, token_to_id\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        assert tokenizer_tgt.token_to_id(\"[EOS]\") is not None, \"[EOS] not found in tokenizer\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_target_pair = self.dataset[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids #input ids as array\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        #padding tokens used to match seq_len\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 #2 for the SOS and EOS tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 #Decoder only has SOS, and target only has EOS\n",
    "\n",
    "        #make sure padding token length is sufficient/non-negative\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "        \n",
    "\n",
    "        #one sentence goes to encoder input, one goes to decoder input, one output sentence\n",
    "        #Add SOS and EOS to source text\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "                ]\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(  #no EOS here, only SOS\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        label = torch.cat(  #add EOS to decoder output\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, #(seq_len)\n",
    "            \"decoder_input\": decoder_input, #(seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len) | mask: these tokens should not be seen by the attention mechanism\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len) | causal mask, so that the decoder doesn't look at next position inputs\n",
    "            \"label\": label, # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n",
    "\n",
    "def causal_mask(size): #so that the decoder doesn't look at next position inputs\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int) #grabs upper triangular matrix, and makes it 0 | upper triangular matrix shows next position inputs\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dce7bc",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "- Download dataset \n",
    "- Create test dataset, val dataset, \n",
    "- Send to DataLoader to be used in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7afdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(config):\n",
    "    dataset_raw = load_dataset('Helsinki-NLP/opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train') #dataset and subset, ex: en-es\n",
    "\n",
    "    #build tokenizer\n",
    "    tokenizer_src = build_tokenizer(config=config, \n",
    "                                           dataset=dataset_raw,\n",
    "                                           lang = config[\"lang_src\"])\n",
    "    tokenizer_tgt = build_tokenizer(config=config, \n",
    "                                           dataset=dataset_raw,\n",
    "                                           lang = config[\"lang_tgt\"])\n",
    "    \n",
    "    #making custom split for training and validation (90-10)\n",
    "    train_dataset_size = int(0.9 * len(dataset_raw))\n",
    "    val_dataset_size = len(dataset_raw) - train_dataset_size\n",
    "    train_dataset_raw, val_dataset_raw = random_split(dataset_raw, [train_dataset_size, val_dataset_size])\n",
    "\n",
    "    train_dataset = TranslationDataset(dataset=train_dataset_raw, \n",
    "                                       tokenizer_src=tokenizer_src,\n",
    "                                       tokenizer_tgt=tokenizer_tgt,\n",
    "                                       src_lang=config['lang_src'],\n",
    "                                       tgt_lang=config['lang_tgt'],\n",
    "                                       seq_len=config['seq_len'])\n",
    "    \n",
    "    val_dataset = TranslationDataset(dataset=val_dataset_raw, \n",
    "                                       tokenizer_src=tokenizer_src,\n",
    "                                       tokenizer_tgt=tokenizer_tgt,\n",
    "                                       src_lang=config['lang_src'],\n",
    "                                       tgt_lang=config['lang_tgt'],\n",
    "                                       seq_len=config['seq_len'])\n",
    "    \n",
    "    #setting max_len based on the dataset src and tgt lengths\n",
    "    max_len_src, max_len_tgt = 0, 0\n",
    "\n",
    "    for item in dataset_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    #Create DataLoaders\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True) #batch size 1 to process each sentence one by one\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d310b",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "Based on vocab_size, it builds the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba4d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(src_vocab_size=vocab_src_len,\n",
    "                              tgt_vocab_size=vocab_tgt_len,\n",
    "                              src_seq_len=config['seq_len'],\n",
    "                              tgt_seq_len=config['seq_len'],\n",
    "                              d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef781eae",
   "metadata": {},
   "source": [
    "## Build Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f714ef7",
   "metadata": {},
   "source": [
    "### Model validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf57fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    #precompute the encoder output and reuse it for every token in the decoder\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    #initialize decoder input with sos token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        #build mask for decoder input\n",
    "        decoder_mask = causal_mask(size=decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        #calculate output of decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        #get next token\n",
    "        prob = model.project(out[:,-1]) #last token\n",
    "        _, next_word = torch.max(prob, dim=1) #selects token with max probability (greedy search)\n",
    "\n",
    "        #next_word becomes input for the next iteration\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return decoder_input.squeeze(0) #remove batch dimension\n",
    "\n",
    "def run_validation(model, validation_dataset, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples = 2):\n",
    "    model.eval()\n",
    "\n",
    "    count = 0\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataset:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) #convert tokens back into text\n",
    "\n",
    "            #save into respective lists\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            #print to console\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'Source: {source_text}')\n",
    "            print_msg(f'Target: {target_text}')\n",
    "            print_msg(f'Predicted: {model_out_text}')\n",
    "\n",
    "            if count == num_examples:\n",
    "                break\n",
    "\n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942e38a",
   "metadata": {},
   "source": [
    "### Model training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "242dbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    #device agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)\n",
    "    model = build_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    \n",
    "    #restore model in case of crash\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f\"Preloading model: {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    #define loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1) #helps with overfitting\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr = config['lr'],\n",
    "                                 eps = 1e-9)\n",
    "    \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        \n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "            model.train()\n",
    "            encoder_input = batch['encoder_input'].to(device) #(batch, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) #(batch, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) #(batch, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) #(batch, 1, seq_len, seq_len)\n",
    "\n",
    "            #run tensors through transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) #(batch, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) #(batch, seq_len, d_model)\n",
    "\n",
    "            proj_output = model.project(decoder_output) #(batch, seq_len, tgt_vocab_size)\n",
    "\n",
    "            label = batch['label'].to(device) #(batch, seq_len)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) #(batch, seq_len, tgt_vocab_size) -> (batch * seq_len, tgt_vocab_size) to compare with label\n",
    "\n",
    "            batch_iterator.set_postfix({f\"Loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            #log the loss\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2a265",
   "metadata": {},
   "source": [
    "Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0468ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 141\n",
      "Max length of target sentence: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|██████████| 197/197 [00:27<00:00,  7.07it/s, Loss=6.100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: Welcome to Baskerville Hall!\"\n",
      "Target: Velkommen til Baskerville herregård!”\n",
      "Predicted: Jeg .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: Our friends had already secured a first-class carriage and were waiting for us upon the platform.\n",
      "Target: Våre venner hadde allerede sikret seg en førsteklasses kupé og stod nå på platformen og ventet på oss.\n",
      "Predicted: Jeg .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saksh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "c:\\Users\\saksh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "c:\\Users\\saksh\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "Processing epoch 01: 100%|██████████| 197/197 [00:27<00:00,  7.12it/s, Loss=5.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: His method had the additional advantage that if they were to take a cab he was all ready to follow them.\n",
      "Target: Hans fremgangsmåte hadde også fordelen at hvis de tok en vogn, ville han dermed også kunne følge etter dem straks.\n",
      "Predicted: Jeg er .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: On the whole I incline to the latter view, since the matter was evidently important, and it is unlikely that the composer of such a letter would be careless.\n",
      "Target: Jeg vil nærmest anta det siste. Saken har åpenbart vært av stor viktighet for ham, og det er ikke sannsynlig at man er skjødesløs når man skal sette sammen et slikt brev.\n",
      "Predicted: Jeg er , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|██████████| 197/197 [00:27<00:00,  7.18it/s, Loss=5.680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"Watson,\" said the baronet, \"it was the cry of a hound.\"\n",
      "Target: “Watson,” sa sir Henry, “det var en hund som ulte.”\n",
      "Predicted: “ Ja , er det det det det det det det det det det .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: \"I hope your visit has cast some light upon those occurrences which have puzzled us?\"\n",
      "Target: “Jeg håper at De ved Deres besøk har hatt anledning til å bringe litt lys over alle de hemmelighetsfulle hendelsene som har satt oss i så stor uro.”\n",
      "Predicted: “ Jeg De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De De\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|██████████| 197/197 [00:27<00:00,  7.15it/s, Loss=5.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: And it was at this moment that there occurred a most strange and unexpected thing. We had risen from our rocks and were turning to go home, having abandoned the hopeless chase.\n",
      "Target: Vi hadde reist oss og skulle gå hjem, da vi fant ut vi måtte oppgi den håpløse jakten.\n",
      "Predicted: Det var , han han han han og han han han han han han han han han han han .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: I whisked round and had just time to catch a glimpse of something which I took to be a large black calf passing at the head of the drive.\n",
      "Target: Jeg snudde meg rundt og så i farten et glimt av noe som jeg tok for en sort kalv som sprang forbi hesten.\n",
      "Predicted: Jeg var jeg jeg jeg var , og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg jeg jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og jeg og , jeg og .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|██████████| 197/197 [00:27<00:00,  7.13it/s, Loss=5.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"It is dead.\"\n",
      "Target: “Den er død.”\n",
      "Predicted: “ Ja , Henry .”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: There remain the people who will actually surround Sir Henry Baskerville upon the moor.\"\n",
      "Target: Men vi har tilbake de personene som nå vil bli sir Henrys omgivelser der ute på moen.”\n",
      "Predicted: Det er , og , og , og , og .”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|██████████| 197/197 [00:27<00:00,  7.12it/s, Loss=5.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: He retained it in his hand after using it to set the hound upon the track.\n",
      "Target: Han holdt den i hånden etter å ha brukt den til å sette hunden på sporet.\n",
      "Predicted: Han var , og han var , og han var .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: It was the sob of a woman, the muffled, strangling gasp of one who is torn by an uncontrollable sorrow.\n",
      "Target: Det var en kvinnes gråt, en undertrykt, kvalt hulking av et menneske som pines av en sorg som det ikke lenger er herre over.\n",
      "Predicted: Det er en , og , og , og , og , og .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|██████████| 197/197 [00:27<00:00,  7.14it/s, Loss=5.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"Well, it is rather obvious.\"\n",
      "Target: “Jo, det er nokså øyensynlig.”\n",
      "Predicted: “ Ja , det er det ikke .”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: I whisked round and had just time to catch a glimpse of something which I took to be a large black calf passing at the head of the drive.\n",
      "Target: Jeg snudde meg rundt og så i farten et glimt av noe som jeg tok for en sort kalv som sprang forbi hesten.\n",
      "Predicted: Jeg var , og jeg var , og jeg var , og jeg var , og jeg var .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|██████████| 197/197 [00:27<00:00,  7.17it/s, Loss=5.194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"It chanced that some little time later Hugo left his guests to carry food and drink--with other worse things, perchance--to his captive, and so found the cage empty and the bird escaped.\n",
      "Target: Da Hugo noe senere forlot sine gjester for å bringe mat og drikke og kanskje endog det som verre var — til sin fange, hendte det seg at han fant buret tomt og fuglen fløyet.\n",
      "Predicted: “ Det var en en en en en , og han var , og han var , og han var , og han var , og han var .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: A lucky long shot of my revolver might have crippled him, but I had brought it only to defend myself if attacked, and not to shoot an unarmed man who was running away.\n",
      "Target: Et heldig skudd av min revolver kunne ha stanset ham, men jeg hadde kun medbrakt den for å forsvare meg selv hvis jeg ble angrepet, og ikke for å skyte en flyktende, ubevæpnet mann.\n",
      "Predicted: Jeg var at jeg ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|██████████| 197/197 [00:27<00:00,  7.13it/s, Loss=4.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: Sherlock Holmes struck his hand against his knee with an impatient gesture.\n",
      "Target: Sherlock Holmes strøk hånden over kneet med en utålmodig bevegelse.\n",
      "Predicted: Holmes og .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: You are not fit for further adventures to-night.\n",
      "Target: De er ikke i stand til å være med på videre eventyr i natt.\n",
      "Predicted: De er at De er Dem .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|██████████| 197/197 [00:27<00:00,  7.10it/s, Loss=4.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: And I would have you believe, my sons, that the same Justice which punishes sin may also most graciously forgive it, and that no ban is so heavy but that by prayer and repentance it may be removed.\n",
      "Target: Og jeg vil, at I, mine sønner, skal tro at den rettferdighet som straffer synden, også nådig tilgir den, og at ingen forbannelse er så tung at den ikke ved bønn og anger kan oppheves.\n",
      "Predicted: Jeg har ikke ikke ikke ikke ikke ikke ikke Dem , og jeg har jeg har være Dem , og jeg har være Dem .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: The young heir glanced round with a gloomy face.\n",
      "Target: Den unge arving så seg omkring med et uttrykk av uhygge.\n",
      "Predicted: og .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|██████████| 197/197 [00:27<00:00,  7.13it/s, Loss=4.840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"Not for the world, my dear Watson.\n",
      "Target: “Nei, ikke for alt i verden, kjære Watson.\n",
      "Predicted: “ Ja , De er Dem Dem , hr .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: And he left five years ago--the date is on the stick.\n",
      "Target: Og han har forlatt sykehuset for fem år siden — årstallet står på stokken.\n",
      "Predicted: Det var han , og han var .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|██████████| 197/197 [00:27<00:00,  7.12it/s, Loss=4.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: We ran and ran until we were completely blown, but the space between us grew ever wider.\n",
      "Target: Vi sprang og sprang, inntil vi var fullstendig utmaset, men avstanden mellom oss og ham ble stadig større og større.\n",
      "Predicted: Vi var en , og vi var en i i i i .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: He certainly seemed to be getting uncomfortably near the truth.\n",
      "Target: Han begynte å nærme seg sannheten på en høyst uhyggelig måte.\n",
      "Predicted: Det er ikke at han ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke ikke .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|██████████| 197/197 [00:27<00:00,  7.17it/s, Loss=4.760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"I suppose it is pretty thick, now that you mention it.\"\n",
      "Target: “Ja, den er visst ganske tykk nå, når De nevner det.”\n",
      "Predicted: “ Nei , jeg har ikke ikke ikke det .”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: He retained it in his hand after using it to set the hound upon the track.\n",
      "Target: Han holdt den i hånden etter å ha brukt den til å sette hunden på sporet.\n",
      "Predicted: Vi kunne ikke ikke ikke ikke ikke ikke ikke ikke ikke se seg .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|██████████| 197/197 [00:27<00:00,  7.17it/s, Loss=4.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: The use of artificial means to make the creature diabolical was a flash of genius upon his part.\n",
      "Target: Bruken av kunstige midler til å gi dyret et djevelsk utseende var et genialt påfunn av ham.\n",
      "Predicted: Vi er en , og det er en , og det er en .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: It was, at least, absolutely effective.\n",
      "Target: Hans innflytelse virket i alle fall.\n",
      "Predicted: Det var en , og det var en .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|██████████| 197/197 [00:27<00:00,  7.13it/s, Loss=4.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: I thought that you were in Baker Street working out that case of blackmailing.\"\n",
      "Target: Jeg trodde at De var i Baker Street og var fullt opptatt av injuriesaken.”\n",
      "Predicted: Jeg har hørt det i å meg i det .”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: I act entirely from a sense of public duty.\n",
      "Target: Jeg handler utelukkende av følelsen av min plikt mot samfunnet.\n",
      "Predicted: Jeg har hørt en i .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15: 100%|██████████| 197/197 [00:27<00:00,  7.15it/s, Loss=4.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: For two hours the strange business in which we had been involved appeared to be forgotten, and he was entirely absorbed in the pictures of the modern Belgian masters.\n",
      "Target: For et par timer var den merkelige saken vi var kommet opp i fullstendig glemt; han gikk helt opp i betraktningen av de moderne belgiske mesteres bilder.\n",
      "Predicted: Han hadde en av , og det var av å være , og .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: \"My word, it does not seem a very cheerful place,\" said the detective with a shiver, glancing round him at the gloomy slopes of the hill and at the huge lake of fog which lay over the Grimpen Mire. \"I see the lights of a house ahead of us.\"\n",
      "Target: “Det er sannelig ikke noe hyggelig ventested,” sa detektiven og fór gysende sammen ved synet av de uhyggelige, stupbratte skrentene og det svære tåkehavet som lå over Grimpenmyren.\n",
      "Predicted: “ Jeg har ikke en av av av , og det , og en , og en , og , som .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 16: 100%|██████████| 197/197 [00:27<00:00,  7.23it/s, Loss=4.356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: My wife and I will be happy, Sir Henry, to stay with you until you have made your fresh arrangements, but you will understand that under the new conditions this house will require a considerable staff.\"\n",
      "Target: Jeg og min hustru vil med glede bli her, til De er kommet i orden, sir Henry, men De forstår at det vil trenges et større tjenerhold til et hus som dette under de nye vilkår.”\n",
      "Predicted: Hvis De har vært , og jeg er det , og jeg er det , og jeg er det i i å gjøre det .”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: \"I hope that you will come also.\n",
      "Target: “Godt.\n",
      "Predicted: “ Jeg har fått redd at De har .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 17: 100%|██████████| 197/197 [00:27<00:00,  7.21it/s, Loss=4.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"You may be right.\"\n",
      "Target: “De har muligens rett.”\n",
      "Predicted: “ De kan være noe til å være mere ?”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: She always comes to us when she is in town.\"\n",
      "Target: Hun bor bestandig her når hun er i byen.”\n",
      "Predicted: Men jeg ville ikke få et av å få et .”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 18: 100%|██████████| 197/197 [00:27<00:00,  7.13it/s, Loss=4.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"I can't give you the name, sir, but I can give you the initials.\n",
      "Target: “Jeg kan ikke oppgi navnet til Dem, sir Henry, men jeg kan si Dem forbokstavene.\n",
      "Predicted: “ Jeg har ikke ikke ikke vært for å fortelle Dem .\n",
      "--------------------------------------------------------------------------------\n",
      "Source: \"And why were you holding a candle to the window?\"\n",
      "Target: “Og hvorfor gjorde De det?”\n",
      "Predicted: “ Hvordan gikk De det ?”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 19: 100%|██████████| 197/197 [00:27<00:00,  7.15it/s, Loss=4.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Source: \"What sort of night was it?'\n",
      "Target: “Hva slags aften var det?”\n",
      "Predicted: “ Er det ?”\n",
      "--------------------------------------------------------------------------------\n",
      "Source: The rattle of our wheels died away as we drove through drifts of rotting vegetation--sad gifts, as it seemed to me, for Nature to throw before the carriage of the returning heir of the Baskervilles.\n",
      "Target: Lyden av vognhjulene døde hen mens vi kjørte gjennom dynger av råtnende blader. Det forekom meg som naturen bød Baskervillernes gjenkomne arving en trist velkomsthilsen.\n",
      "Predicted: Det var en av å være en av en , og vi var kommet av moen , og seg seg seg seg i moen .\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589857ee",
   "metadata": {},
   "source": [
    "Test the trained model with custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f090a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence: str,\n",
    "    model,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    config,\n",
    "    max_len=50,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # Encode source sentence\n",
    "    tokens = tokenizer_src.encode(sentence).ids\n",
    "    tokens = [tokenizer_tgt.token_to_id(\"[SOS]\")] + tokens + [tokenizer_tgt.token_to_id(\"[EOS]\")]\n",
    "\n",
    "    encoder_input = torch.tensor(tokens, dtype=torch.int64).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    encoder_mask = (encoder_input != tokenizer_tgt.token_to_id(\"[PAD]\")).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "\n",
    "    # Prepare decoder input (just SOS at first)\n",
    "    decoder_input = torch.tensor([[tokenizer_tgt.token_to_id(\"[SOS]\")]], dtype=torch.int64).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        decoder_mask = (decoder_input != tokenizer_tgt.token_to_id(\"[PAD]\")).unsqueeze(0).unsqueeze(0)\n",
    "        size = decoder_input.size(1)\n",
    "        causal_mask = torch.triu(torch.ones((1, size, size), device=device), diagonal=1).bool()\n",
    "        decoder_mask = decoder_mask & ~causal_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            logits = model.project(decoder_output)  # (1, seq_len, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]  # last token\n",
    "\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)  # greedy decode\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer_tgt.token_to_id(\"[EOS]\"):\n",
    "            break\n",
    "\n",
    "    # Remove SOS and EOS\n",
    "    output_tokens = decoder_input.squeeze().tolist()\n",
    "    output_tokens = output_tokens[1:]  # remove SOS\n",
    "    if tokenizer_tgt.token_to_id(\"[EOS]\") in output_tokens:\n",
    "        output_tokens = output_tokens[:output_tokens.index(tokenizer_tgt.token_to_id(\"[EOS]\"))]\n",
    "\n",
    "    return tokenizer_tgt.decode(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48f91c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from weights\\transformer1_19.pt\n",
      "→ Det er en av .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = get_config()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer_src = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_src']))\n",
    "tokenizer_tgt = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_tgt']))\n",
    "\n",
    "# Build model\n",
    "model = build_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
    "model.to(device)\n",
    "\n",
    "# Load trained weights\n",
    "model_filename = get_weights_file_path(config, \"19\")  # or last epoch saved\n",
    "print(f\"Loading model from {model_filename}\")\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sentence = \"The weather is nice today.\"\n",
    "translation = translate_sentence(sentence, model, tokenizer_src, tokenizer_tgt, config)\n",
    "print(\"→\", translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
