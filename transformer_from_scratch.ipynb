{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac2d36a",
   "metadata": {},
   "source": [
    "# Transformer using PyTorch\n",
    "\n",
    "Replicating the paper \"Attention Is All You Need\" from scratch using PyTorch.\n",
    "\n",
    "Available here: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdc492",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54f417",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "In NLP, words/tokens are represented as integers (token IDs), but since neural networks cannot work directly with numbers, these are first converted into continuous dense vectors via embeddings.\n",
    "\n",
    "The embedding layer:\n",
    "- Maps each token ID to a learnable vector of dimension `d_model`\n",
    "- Allows the model to learn semantic meaning - similar words/tokens get similar embeddings during training\n",
    "\n",
    "Refer to section 3.4 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #maps token IDs to vectors\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of token indices, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Embedded tensor, scaled, shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)  #Refer section 3.4 of the paper (This is primarily done to balance the scale of the embeddings and positional encodings, and helps stabilize training early on.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238030c",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Transformers process tokens in parallel, with no built-in sense of order. But language is sequential:\n",
    "\n",
    "\"The cat sat on the mat\" =/= \"The mat cat on the sat\"\n",
    "\n",
    "Positional information is injected into the input to create awareness of the token position, using Positional Encoding. They have the same dimension `d_model` as the input embeddings.\n",
    "\n",
    "Refer to section 3.5 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3212ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Create matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #Create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) #tensor contains positions 0,1,...,seq_len - 1\n",
    "        #Using section 3.5 to write out position encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #sine to even positions\n",
    "        pe[:, 1::2] = torch.sin(position * div_term) #cosine to odd positions\n",
    "\n",
    "        #Add batch dimension to make the shape (1, seq_len, d_model), so it can be added to x during forward pass\n",
    "        #with shape (batch_size, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe) #tensor gets saved with the model and moves with it to GPU/CPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape(1), :]).requires_grad_(False) #Add positional encodings to embeddings, requires_grad_() False means that this won't participate in backpropagation\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b617ed",
   "metadata": {},
   "source": [
    "### Add + Norm Layer\n",
    "\n",
    "- Layer Norm normalizes each sampleâ€™s hidden vector across dimensions, improving training dynamics and convergence.\n",
    "- LayerNorm is preferred over BatchNorm because:\n",
    "    1. It doesn't depend on batch statistics.\n",
    "    2. Works well for variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12840f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #added (shift)\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) #multiplied (scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.epsilon) + self.bias #From section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d4584",
   "metadata": {},
   "source": [
    "### Feed Forward layer\n",
    "\n",
    "Discussed in section 3.3 of the paper, and equation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ac9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) #W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) #W2 and B2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x)))) #(batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023eb53",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Discussed in section 3.2.2 of the paper, this is the core innovation of the transformer model.\n",
    "It is multiple scaled dot-product attention heads run in parallel, followed by a linear projection.\n",
    "\n",
    "Refer to equation 1 for calculating attention scores, and then two equations in section 3.2.2 for multihead attention scores\n",
    "\n",
    "Steps involved:\n",
    "1. Project input `(Q, K, V)` into h subspaces using `W_q`, `W_k`, `W_v`.\n",
    "2. Compute scaled dot-product attention for each head.\n",
    "3. Concatenate all head outputs.\n",
    "4. Pass through a final `W_o` projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout): #h is the number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model #we have to ensure that d_model is divisible by h, d_model/h = d_k\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model) #W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) #W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) #W_v\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) #W_o\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1] #last dimension of Q,K,V\n",
    "\n",
    "        # Calculate attention using Equation 1\n",
    "        # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) #matrix multiplication of Q with K.transpose\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) #values we don't want in the attention matrix will be replaced by a very low value\n",
    "        \n",
    "        # apply softmax\n",
    "        attention_scores = attention_scores.softmax(dim = -1) #(batch, h, seq_len, seq_len)\n",
    "\n",
    "        # apply dropout\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)       # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)     # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    " \n",
    "        # Reshaping for MultiHead Attention from [batch, seq_len, d_model] to [batch, seq_len, h, d_k]\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) #(batch, seq_len, d_model) -> (batch, seq_len, h, d_k).transpose -> (batch, seq_len, d_model) -> (batch, h, seq_len, d_k)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) #revert the transpose operation (batch, h, seq_len, d_k).transpose -> (batch, seq_len, d_model)\n",
    "        \n",
    "        #(batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fad4",
   "metadata": {},
   "source": [
    "### Create skip connection (residual connection)\n",
    "\n",
    "Addition is element-wise, and the result layer is normalized\n",
    "\n",
    "`Output = LayerNorm(x + Sublayer(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    #Applying Post-Norm here\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e6c1d",
   "metadata": {},
   "source": [
    "## Creating Encoder block\n",
    "Creating the Encoder block, and stacking N of them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_block: MultiHeadAttention, \n",
    "                 feed_forward_block: FeedForward,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([SkipConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask): #mask to hide padding words\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e7090",
   "metadata": {},
   "source": [
    "### Creating the Encoder object (N encoder blocks stacked)\n",
    "\n",
    "Each block:\n",
    "- Applies multi-head attention to every position with all other positions (self-attention)\n",
    "- Applies a feedforward network\n",
    "- Each sub-layer is wrapped with residual + layer norm\n",
    "\n",
    "After all blocks, it applies a final layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef379cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:  #Loop over each EncoderBlock in self.layers\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747610e",
   "metadata": {},
   "source": [
    "## Creating Decoder Block\n",
    "\n",
    "Uses the same classes as the Encoder block for the most part.\n",
    "The key difference comes in with the Masked Multi-Head Attention (uses self-attention), and an additional, cross-attention Multi-Head Attention block.\n",
    "\n",
    "Refer to section 3.1 in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c494dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_attention_block: MultiHeadAttention,\n",
    "                 cross_attention_block: MultiHeadAttention,\n",
    "                 feed_forward_block: FeedForward,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        self.residual_connections = nn.ModuleList([SkipConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask): #src_mask: encoder, tgt_mask: decoder\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask)) #self-attention block of decoder\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)) #cross-attention takes in key from the decoder, and the query and value from the encoder, with the encoder mask\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc81f44",
   "metadata": {},
   "source": [
    "### Creating Decoder object (N decoder blocks stacked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50318473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76031cb8",
   "metadata": {},
   "source": [
    "## Final Linear Layer\n",
    "Projects the embeddings into a position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #(batch, seq_len, d_model) -> (batch, seq_len, vocab_size)\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe16925",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "We now have all the components we need to put together and create the Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embed: InputEmbeddings,\n",
    "                 tgt_embed: InputEmbeddings,\n",
    "                 src_pos: PositionalEncoding,\n",
    "                 tgt_pos: PositionalEncoding,\n",
    "                 projection_layer: ProjectionLayer\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    #define 3 methods: one to encode, one to decode, one to project\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)   #apply embedding\n",
    "        src = self.src_pos(src)     #apply positional encoding\n",
    "        return self.encoder(src, src_mask)  #apply encoder block\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)   #apply embedding\n",
    "        tgt = self.tgt_pos(tgt)     #apply positional encoding\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c456771",
   "metadata": {},
   "source": [
    "### Function to build transformer\n",
    "\n",
    "Given all hyperparameters, this function builds the transformer and initializes the parameters with some initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int,\n",
    "                      tgt_vocab_size: int,\n",
    "                      src_seq_len: int,\n",
    "                      tgt_seq_len: int,\n",
    "                      d_model: int = 512,   #as mentioned in the paper\n",
    "                      N: int = 6,           #number of encoder/decoder blocks\n",
    "                      h: int = 8,           #number of heads\n",
    "                      dropout: float = 0.1,\n",
    "                      d_ff: int = 2048\n",
    ") -> Transformer:\n",
    "    \n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    #create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    #create decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForward(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, \n",
    "                                     decoder_cross_attention_block,\n",
    "                                     feed_forward_block,\n",
    "                                     dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    #Create the encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #Create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    #Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    #Initialize parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f382d",
   "metadata": {},
   "source": [
    "# Creating English - Spanish translation Transformer\n",
    "\n",
    "1. Using opus_books dataset from HuggingFace (Available here: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-es?views%5B%5D=en_es)\n",
    "2. Use word-level tokenizer from HuggingFace - Splits sentence into tokens (Build vocabulary) (Documentation here: https://github.com/huggingface/tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc443f",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096185d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, lang):\n",
    "    for item in dataset:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, dataset, lang):\n",
    "    #config['tokenizer_file'] = '../tokenizers/tokenizer_{0}.json'\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) #tokenizes unknown word\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"EOS\"], min_frequency=2) #unknown word, padding, start of sentenece, end of sentence | min_frequency: for the word to appear in the vocabulary it needs to appear at least twice\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a24c9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Custom Dataset Class\n",
    "\n",
    "Use a custom Dataset class, following PyTorch documentation: https://docs.pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        #Save tokens to create tensors for the model: SOS, EOS and Padding\n",
    "        #Use special method of tokenizer, token_to_id\n",
    "\n",
    "        self.sos_token = torch.Tensor(tokenizer_src.token_to_id(['[SOS]']), dtype=torch.int64)\n",
    "        self.eos_token = torch.Tensor(tokenizer_src.token_to_id(['[EOS]']), dtype=torch.int64)\n",
    "        self.pad_token = torch.Tensor(tokenizer_src.token_to_id(['[PAD]']), dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_target_pair = self.dataset[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids #input ids as array\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        #padding tokens used to match seq_len\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 #2 for the SOS and EOS tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 #Decoder only has SOS, and target only has EOS\n",
    "\n",
    "        #make sure padding token length is sufficient/non-negative\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "        \n",
    "\n",
    "        #one sentence goes to encoder input, one goes to decoder input, one output sentence\n",
    "        #Add SOS and EOS to source text\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "                ]\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(  #no EOS here, only SOS\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        decoder_output = torch.cat(  #add EOS to decoder output\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert decoder_output.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, #(seq_len)\n",
    "            \"decoder_input\": decoder_input, #(seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len) | mask: these tokens should not be seen by the attention mechanism\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len) | causal mask, so that the decoder doesn't look at next position inputs\n",
    "            \"decoder_output\": decoder_output, # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n",
    "\n",
    "def causal_mask(size): #so that the decoder doesn't look at next position inputs\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int) #grabs upper triangular matrix, and makes it 0 | upper triangular matrix shows next position inputs\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dce7bc",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "\n",
    "- Download dataset \n",
    "- Create test dataset, val dataset, \n",
    "- Send to DataLoader to be used in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(config):\n",
    "    dataset_raw = load_dataset('Helsinki-NLP/opus_books', f'{config[\"lang_src\"]} - {config[\"lang_tgt\"]}', split='train') #dataset and subset, ex: en-es\n",
    "\n",
    "    #build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config=config, \n",
    "                                           dataset=dataset_raw,\n",
    "                                           lang = config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config=config, \n",
    "                                           dataset=dataset_raw,\n",
    "                                           lang = config[\"lang_tgt\"])\n",
    "    \n",
    "    #making custom split for training and validation (90-10)\n",
    "    train_dataset_size = int(0.9 * len(dataset_raw))\n",
    "    val_dataset_size = len(dataset_raw) - train_dataset_size\n",
    "    train_dataset_raw, val_dataset_raw = random_split(dataset_raw, [train_dataset_size, val_dataset_size])\n",
    "\n",
    "    train_dataset = TranslationDataset(dataset=train_dataset_raw, \n",
    "                                       tokenizer_src=tokenizer_src,\n",
    "                                       tokenizer_tgt=tokenizer_tgt,\n",
    "                                       src_lang=config['lang_src'],\n",
    "                                       tgt_lang=config['lang_tgt'],\n",
    "                                       seq_len=config['seq_len'])\n",
    "    \n",
    "    val_dataset = TranslationDataset(dataset=val_dataset_raw, \n",
    "                                       tokenizer_src=tokenizer_src,\n",
    "                                       tokenizer_tgt=tokenizer_tgt,\n",
    "                                       src_lang=config['lang_src'],\n",
    "                                       tgt_lang=config['lang_tgt'],\n",
    "                                       seq_len=config['seq_len'])\n",
    "    \n",
    "    #setting max_len based on the dataset src and tgt lengths\n",
    "    max_len_src, max_len_tgt = 0, 0\n",
    "\n",
    "    for item in dataset_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n",
    "\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    #Create DataLoaders\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True) #batch size 1 to process each sentence one by one\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d310b",
   "metadata": {},
   "source": [
    "### Build model\n",
    "\n",
    "Based on vocab_size, it builds the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
